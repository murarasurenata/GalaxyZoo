{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import csv\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Prelucrare imagini:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mypath = \"images_test_rev1/\"\n",
    "newPath = \"test_small3/\"\n",
    "files = listdir(mypath)\n",
    "newFiles = listdir(newPath)\n",
    "\n",
    "for id in range(len(files)):\n",
    "    path = mypath + str(files[id])\n",
    "    try:\n",
    "        img2 = Image.open(path).convert('L');  #greyscale\n",
    "        half_the_width = img.size[0] / 2\n",
    "        half_the_height = img.size[1] / 2\n",
    "        img3 = img2.crop(\n",
    "            (\n",
    "                half_the_width - 100,\n",
    "                half_the_height - 100,\n",
    "                half_the_width + 100,\n",
    "                half_the_height + 100\n",
    "            )\n",
    "        )     \n",
    "        img4 = img3.resize([80,80]) \n",
    "        file_name = str(files[id]);\n",
    "        img4.save(newPath + file_name) #salveazÄƒ imaginea \n",
    "        if (id%500 == 0):\n",
    "            gc.collect()\n",
    "    except IOError as e:\n",
    "      print('Could not read:', file_name, ':', e, '- it\\'s ok, skipping.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79975\n",
      "79975\n"
     ]
    }
   ],
   "source": [
    "mypath = \"images_test_rev1/\"\n",
    "newPath = \"test_small3/\"\n",
    "files = listdir(mypath)\n",
    "print(len(files))\n",
    "newFiles = listdir(newPath)\n",
    "print(len(newFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\test_small3\n",
      "Pickling .\\test_small31.pickle.\n",
      ".\\test_small3\n",
      "Full dataset tensor: (79975, 80, 80)\n",
      "Mean: -0.405905\n",
      "Standard deviation: 0.128175\n"
     ]
    }
   ],
   "source": [
    "image_size = 80  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "test_folder = '.\\\\test_small3'\n",
    "\n",
    "def load_images(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single  folder.\"\"\"\n",
    "\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  ids = np.ndarray(shape=(len(image_files)), dtype=(np.str_, 6))\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      ids[num_images] = image\n",
    "      #print(ids[num_images])\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  ids = ids[0:num_images]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset, ids\n",
    "        \n",
    "def maybe_pickle(folder, min_num_images_per_class, force=False):\n",
    "    dataset_names = []\n",
    "\n",
    "    print(folder)\n",
    "\n",
    "    set_filename = folder + '1.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      #override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset, ids = load_images(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          save = {\n",
    "            'dataset': dataset,\n",
    "            'ids' : ids\n",
    "            }\n",
    "          pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "    return dataset_names\n",
    "  \n",
    "\n",
    "train_datasets = maybe_pickle(test_folder, 79975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Generare CSV cu rezultate:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image_size = 80\n",
    "num_labels = 37\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "def reformat(dataset):\n",
    "  dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  return dataset\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return np.sqrt(((predictions - labels) ** 2).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set (79975, 80, 80) (79975,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'test_small31.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  test_dataset = (save['dataset'])\n",
    "  test_id = (save['ids'])\n",
    "\n",
    "  del save  # hint to help gc free up memory\n",
    "    \n",
    "  print('Test set', test_dataset.shape, test_id.shape)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set (79975, 80, 80, 1) (79975,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_dataset = reformat(test_dataset)\n",
    "\n",
    "print('Test set', test_dataset.shape, test_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "#   MODEL 5\n",
    "batch_size = 128 #109\n",
    "patch_size = 5\n",
    "depth = 32 #32, 16\n",
    "num_hidden = 64 #100\n",
    "num_hidden2 = 64 #37\n",
    "beta = 0.0002#0.0002\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=sqrt(2.0/(image_size // 4 * image_size // 4 * depth * num_hidden))))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  print(str(num_hidden))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_hidden2], stddev=sqrt(2.0/(num_hidden * num_hidden2))))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden2]))\n",
    "  print(str(num_hidden2))\n",
    "  layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden2, num_labels], stddev=sqrt(2.0/(num_hidden2 * num_labels))))\n",
    "  layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv + layer1_biases)\n",
    "    hidden =  tf.nn.max_pool(hidden1, [1,2,2,1], [1,2,2,1],padding='SAME')\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv + layer2_biases)\n",
    "    hidden =  tf.nn.max_pool(hidden1, [1,2,2,1], [1,2,2,1],padding='SAME')\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden, layer4_weights) + layer4_biases)\n",
    "    return tf.matmul(hidden2, layer5_weights) + layer5_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss1 = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(logits, tf_train_labels))))\n",
    "  loss = loss1 + beta * (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer5_weights))\n",
    "    \n",
    " # Optimizer.\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  #learning_rate = tf.train.exponential_decay(0.3, global_step, 5000 ,0.96, staircase = True) #5000\n",
    "  learning_rate = tf.Variable(0.3)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "   \n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  def prediction(logits):\n",
    "    pred1 = tf.nn.softmax(logits[:,0:3])\n",
    "    pred2 = tf.nn.relu(logits[:,3:num_labels])\n",
    "    pred = tf.concat([pred1, pred2],1)\n",
    "    return pred\n",
    "  train_prediction1 = prediction(logits)\n",
    "   #drop out some of the predictions\n",
    "  train_prediction = tf.nn.dropout(train_prediction1,0.5)\n",
    "  \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "  tf.add_to_collection('vars',  layer1_weights)\n",
    "  tf.add_to_collection('vars',  layer2_weights)\n",
    "  tf.add_to_collection('vars',  layer3_weights)\n",
    "  tf.add_to_collection('vars',  layer4_weights)\n",
    "  tf.add_to_collection('vars',  layer5_weights)\n",
    "  tf.add_to_collection('vars',  layer1_biases)\n",
    "  tf.add_to_collection('vars',  layer2_biases)\n",
    "  tf.add_to_collection('vars',  layer3_biases)\n",
    "  tf.add_to_collection('vars',  layer4_biases)\n",
    "  tf.add_to_collection('vars',  layer5_biases)\n",
    "  saver = tf.train.Saver(write_version=tf.train.SaverDef.V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "Initialized\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "results = np.ndarray(shape=(test_id.shape[0],38))\n",
    "acc = 0\n",
    "t = []\n",
    "#with open(filename, 'w') as myfile:\n",
    "    #wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    saver.restore(session, './my-model5.ckpt')\n",
    "    print(\"Model restored.\") \n",
    "    print('Initialized')\n",
    "    for i in range(0,79):\n",
    "        results[i*1000:(i+1)*1000,0] = test_id[i*1000:(i+1)*1000]\n",
    "        results[i*1000:(i+1)*1000,1:38] = prediction(model(test_dataset[i*1000:(i+1)*1000,:,:,:])).eval()\n",
    "        #for j in range(0,37):\n",
    "         #   a.append(pre[0,j])\n",
    "        \n",
    "        gc.collect()\n",
    "        print(i)\n",
    "            #wr.writerow(a)\n",
    "    results[79*1000:test_id.shape[0],0] = test_id[79*1000:test_id.shape[0]]\n",
    "    results[79*1000:test_id.shape[0],1:38] = prediction(model(test_dataset[79*1000:test_id.shape[0],:,:,:])).eval()\n",
    "    np.savetxt(\"results2.csv\", results, delimiter=\",\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
