{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import csv\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mypath = \"images_training_rev1/\"\n",
    "newPath = \"training_small3/\"\n",
    "files = listdir(mypath)\n",
    "newFiles = listdir(newPath)\n",
    "\n",
    "for id in range(len(files)):\n",
    "    path = mypath + str(files[id])\n",
    "    try:\n",
    "        img = Image.open(path).convert('L');  #greyscale\n",
    "        half_the_width = img.size[0] / 2\n",
    "        half_the_height = img.size[1] / 2\n",
    "        for i in range(0,4):\n",
    "            degree = i*45;\n",
    "            img2 = img.rotate(degree) \n",
    "            img3 = img2.crop(\n",
    "                (\n",
    "                    half_the_width - 100,\n",
    "                    half_the_height - 100,\n",
    "                    half_the_width + 100,\n",
    "                    half_the_height + 100\n",
    "                )\n",
    "            )     \n",
    "            img4 = img3.resize([80,80]) \n",
    "            file_name = str(files[id]);\n",
    "            img4.save(newPath + file_name[0] + \"/\" + str(i) + \"_\" + file_name) #salveazÄƒ imaginea \n",
    "        if (id%500 == 0):\n",
    "            gc.collect()\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61578\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "mypath = \"images_training_rev1/\"\n",
    "newPath = \"training_80px/\"\n",
    "files = listdir(mypath)\n",
    "print(len(files))\n",
    "newFiles = listdir(newPath)\n",
    "print(len(newFiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "v = np.empty((1000000, 0)).tolist()\n",
    "with open('training_solutions_rev1.csv', 'rt') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    first = True;\n",
    "    for row in spamreader:\n",
    "        if(not(first)):\n",
    "            v[int(row[0])] = list(map(float, row[1:len(row)]))\n",
    "        else:\n",
    "            first = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\training_80px\\1\n",
      "Pickling .\\training_80px\\1.pickle.\n",
      ".\\training_80px\\1\n",
      "[ 0.605956    0.392745    0.001299    0.          0.392745    0.12399942\n",
      "  0.26874559  0.14848864  0.24425636  0.          0.          0.24732134\n",
      "  0.14542366  0.581634    0.418366    0.10828737  0.49766863  0.          0.\n",
      "  0.          0.          0.0343199   0.21641496  0.33089914  0.          0.\n",
      "  0.          0.          0.          0.14848864  0.          0.\n",
      "  0.14848864  0.          0.          0.          0.        ]\n",
      "Full dataset tensor: (27044, 80, 80)\n",
      "Mean: -0.406139\n",
      "Standard deviation: 0.127839\n",
      ".\\training_80px\\2\n",
      "Pickling .\\training_80px\\2.pickle.\n",
      ".\\training_80px\\2\n",
      "[ 0.321804    0.678196    0.          0.678196    0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.051974\n",
      "  0.948026    0.          0.035876    0.285928    0.051974    0.          0.\n",
      "  0.          0.          0.          0.          0.35133537  0.\n",
      "  0.32686063  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "Full dataset tensor: (26840, 80, 80)\n",
      "Mean: -0.405039\n",
      "Standard deviation: 0.128974\n",
      ".\\training_80px\\3\n",
      "Pickling .\\training_80px\\3.pickle.\n",
      ".\\training_80px\\3\n",
      "[ 0.208       0.792       0.          0.          0.792       0.0792\n",
      "  0.7128      0.792       0.          0.          0.6732      0.1188      0.\n",
      "  0.129       0.871       0.158288    0.049712    0.          0.          0.\n",
      "  0.07928533  0.          0.0396426   0.01007206  0.          0.          0.\n",
      "  0.          0.5544      0.2376      0.          0.          0.3564\n",
      "  0.2772      0.0396      0.0396      0.0792    ]\n",
      "Full dataset tensor: (28164, 80, 80)\n",
      "Mean: -0.406093\n",
      "Standard deviation: 0.128146\n",
      ".\\training_80px\\4\n",
      "Pickling .\\training_80px\\4.pickle.\n",
      ".\\training_80px\\4\n",
      "[ 0.847634    0.13074     0.021626    0.          0.13074     0.          0.13074\n",
      "  0.          0.13074     0.          0.          0.13074     0.          0.106146\n",
      "  0.893854    0.20088926  0.64674474  0.          0.01770866  0.07072869\n",
      "  0.          0.          0.01770866  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "Full dataset tensor: (27176, 80, 80)\n",
      "Mean: -0.406535\n",
      "Standard deviation: 0.127738\n",
      ".\\training_80px\\5\n",
      "Pickling .\\training_80px\\5.pickle.\n",
      ".\\training_80px\\5\n",
      "[ 0.293563    0.675203    0.031234    0.06407204  0.61113096  0.09538226\n",
      "  0.5157487   0.49949995  0.11163102  0.          0.1053932   0.47532605\n",
      "  0.03041232  0.134489    0.865511    0.01649442  0.25822183  0.01884675\n",
      "  0.          0.          0.0806934   0.0268978   0.0268978   0.          0.\n",
      "  0.06407204  0.          0.          0.22791733  0.23274749  0.03883512\n",
      "  0.10210778  0.20543883  0.          0.          0.          0.19195333]\n",
      "Full dataset tensor: (27456, 80, 80)\n",
      "Mean: -0.406449\n",
      "Standard deviation: 0.127577\n",
      ".\\training_80px\\6\n",
      "Pickling .\\training_80px\\6.pickle.\n",
      ".\\training_80px\\6\n",
      "[ 0.052812    0.947188    0.          0.          0.947188    0.25231099\n",
      "  0.69487701  0.57381217  0.37337583  0.04348635  0.64077742  0.21637279\n",
      "  0.0465524   0.404905    0.595095    0.04141485  0.01139715  0.\n",
      "  0.28939168  0.          0.028777    0.08673632  0.          0.          0.\n",
      "  0.          0.          0.          0.36605257  0.2077596   0.\n",
      "  0.09618756  0.26398344  0.          0.          0.          0.2136406 ]\n",
      "Full dataset tensor: (27836, 80, 80)\n",
      "Mean: -0.405182\n",
      "Standard deviation: 0.128541\n",
      ".\\training_80px\\7\n",
      "Pickling .\\training_80px\\7.pickle.\n",
      ".\\training_80px\\7\n",
      "[ 0.591926    0.392031    0.016043    0.          0.392031    0.          0.392031\n",
      "  0.          0.392031    0.10906498  0.19346142  0.0895046   0.          0.107564\n",
      "  0.892436    0.          0.31993659  0.27198941  0.          0.          0.053782\n",
      "  0.          0.053782    0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.        ]\n",
      "Full dataset tensor: (27760, 80, 80)\n",
      "Mean: -0.405347\n",
      "Standard deviation: 0.128579\n",
      ".\\training_80px\\8\n",
      "Pickling .\\training_80px\\8.pickle.\n",
      ".\\training_80px\\8\n",
      "[ 0.364114    0.603475    0.032411    0.          0.603475    0.          0.603475\n",
      "  0.          0.603475    0.          0.06443303  0.4888588   0.05018317\n",
      "  0.565317    0.434683    0.30784965  0.05626435  0.          0.          0.\n",
      "  0.          0.02395813  0.18252503  0.3588344   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.        ]\n",
      "Full dataset tensor: (26804, 80, 80)\n",
      "Mean: -0.405556\n",
      "Standard deviation: 0.128449\n",
      ".\\training_80px\\9\n",
      "Pickling .\\training_80px\\9.pickle.\n",
      ".\\training_80px\\9\n",
      "[ 0.675441    0.324559    0.          0.          0.324559    0.          0.324559\n",
      "  0.          0.324559    0.          0.12295269  0.20160631  0.          0.\n",
      "  1.          0.07839979  0.59704121  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.        ]\n",
      "Full dataset tensor: (27232, 80, 80)\n",
      "Mean: -0.405246\n",
      "Standard deviation: 0.128553\n"
     ]
    }
   ],
   "source": [
    "image_size = 80  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "train_folders = ['.\\\\training_80px\\\\1', '.\\\\training_80px\\\\2', '.\\\\training_80px\\\\3', '.\\\\training_80px\\\\4', '.\\\\training_80px\\\\5', '.\\\\training_80px\\\\6', '.\\\\training_80px\\\\7', '.\\\\training_80px\\\\8', '.\\\\training_80px\\\\9']\n",
    "\n",
    "def load_images(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single  folder.\"\"\"\n",
    "\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "\n",
    "  labels = np.ndarray(shape=(len(image_files),37))\n",
    "  ids = np.ndarray(shape=(len(image_files)))\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      idstr = str(image).replace(\".jpg\",\"\").replace(\"0_\",\"\").replace(\"1_\",\"\").replace(\"2_\",\"\").replace(\"3_\",\"\")\n",
    "      id = int(idstr)\n",
    "      ids[num_images] = id\n",
    "      labels[num_images] = v[id]\n",
    "      if (num_images == 500): \n",
    "        print(labels[num_images])\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset, labels, ids\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    print(folder)\n",
    "    \n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      #override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset, labels, ids = load_images(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          save = {\n",
    "            'dataset': dataset,\n",
    "            'labels': labels,\n",
    "            'ids': ids,\n",
    "            }\n",
    "          pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "  \n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b, c):\n",
    "    assert len(a) == len(b)\n",
    "    assert len(a) == len(c)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p], c[p]\n",
    "\n",
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray((nb_rows, 37), dtype=np.float32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0, test_size=0 ):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  test_dataset, test_labels = make_arrays(test_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "  ttsize_per_class = test_size // num_classes\n",
    "  start_v, start_t, start_tt = 0, 0, 0\n",
    "  end_v, end_t, end_tt = vsize_per_class, tsize_per_class, ttsize_per_class\n",
    "  end_l = vsize_per_class + tsize_per_class + ttsize_per_class\n",
    "\n",
    "  for fd, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        save = pickle.load(f)  # unpickle\n",
    "        dataset = save['dataset']\n",
    "        labels = save['labels']\n",
    "        ids = save['ids']\n",
    "        # shuffle the images to have random validation and training set\n",
    "        unison_shuffled_copies(dataset, labels, ids)\n",
    "        \n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = dataset[:vsize_per_class, :, :]\n",
    "          print(len(valid_letter))\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v,:] = labels[:vsize_per_class, :]\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "        \n",
    "        if test_dataset is not None:\n",
    "          test_letter = dataset[vsize_per_class:vsize_per_class+ttsize_per_class, :, :]\n",
    "          print(len(test_letter))\n",
    "          test_dataset[start_tt:end_tt, :, :] = test_letter\n",
    "          test_labels[start_tt:end_tt] = labels[vsize_per_class:vsize_per_class+ttsize_per_class, :]\n",
    "          start_tt += ttsize_per_class\n",
    "          end_tt += ttsize_per_class\n",
    "        \n",
    "        train_letter = dataset[vsize_per_class+ttsize_per_class:end_l, :, :]\n",
    "\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = labels[vsize_per_class+ttsize_per_class:end_l, :]\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels \n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 20000\n",
    "test_size = 20000\n",
    "\n",
    "train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size, test_size)\n",
    "\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_root = '.'\n",
    "pickle_file = os.path.join(data_root, 'galaxies.pickle')\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# overlaps between training and test sets:1 execution time: 15.883333320164631\n",
      "# overlaps between training and validation sets:1execution time:5.3156518468711536\n",
      "# overlaps between validation and test sets:1execution time:0.6824502795847422\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def check_overlaps(images1, images2):\n",
    "    start = time.clock()\n",
    "    hash1 = set([hash(image1.tobytes()) for image1 in images1])\n",
    "    hash2 = set([hash(image2.tobytes()) for image2 in images2])\n",
    "    all_overlaps = set.intersection(hash1, hash2)\n",
    "    return all_overlaps, time.clock()-start\n",
    "\n",
    "r, execTime = check_overlaps(train_dataset, test_dataset)    \n",
    "print(\"# overlaps between training and test sets:\" + str(len(r)) + \" execution time: \" + str(execTime))\n",
    "r, execTime = check_overlaps(train_dataset, valid_dataset)   \n",
    "print(\"# overlaps between training and validation sets:\"+ str(len(r))+ \"execution time:\"+str(execTime))\n",
    "r, execTime = check_overlaps(valid_dataset, test_dataset) \n",
    "print(\"# overlaps between validation and test sets:\"+ str(len(r))+ \"execution time:\"+ str(execTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
